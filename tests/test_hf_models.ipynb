{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, T5EncoderModel, T5Config, T5Model, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 60.506624M\n"
     ]
    }
   ],
   "source": [
    "encoder = T5EncoderModel.from_pretrained('t5-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "print(f\"Number of parameters in model: {model.num_parameters()/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state'])\n",
      "torch.Size([1, 7, 512])\n",
      "odict_keys(['last_hidden_state', 'past_key_values'])\n",
      "tensor([[-11.4802,  -6.3112,  -8.8519,  ..., -40.1647, -40.2136, -40.1873],\n",
      "        [-24.6288,  -8.8084,  -9.6085,  ..., -43.8241, -43.8325, -43.7878],\n",
      "        [-25.8387,  -6.6168, -10.3323,  ..., -40.9006, -40.9082, -40.9096],\n",
      "        ...,\n",
      "        [-28.0055,  -6.4625, -10.2796,  ..., -46.8775, -46.9684, -46.9990],\n",
      "        [-26.3766,  -7.5304, -11.4456,  ..., -44.3888, -44.4481, -44.4851],\n",
      "        [-28.7336,  -3.8104, -10.0482,  ..., -46.8083, -46.8263, -46.8783]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "odict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])\n",
      "tensor([[-11.4802,  -6.3112,  -8.8519,  ..., -40.1647, -40.2136, -40.1873],\n",
      "        [-24.6288,  -8.8084,  -9.6085,  ..., -43.8241, -43.8325, -43.7878],\n",
      "        [-25.8387,  -6.6168, -10.3323,  ..., -40.9006, -40.9082, -40.9096],\n",
      "        ...,\n",
      "        [-28.0055,  -6.4625, -10.2796,  ..., -46.8775, -46.9684, -46.9990],\n",
      "        [-26.3766,  -7.5304, -11.4456,  ..., -44.3888, -44.4481, -44.4851],\n",
      "        [-28.7336,  -3.8104, -10.0482,  ..., -46.8083, -46.8263, -46.8783]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 7, 32128])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "encoder_outputs = model.encoder(**inputs)\n",
    "print(encoder_outputs.keys())\n",
    "print(encoder_outputs.last_hidden_state.shape)\n",
    "decoder_inputs = {k:model._shift_right(v) for k, v in inputs.items()}\n",
    "decoder_outputs = model.decoder(input_ids=decoder_inputs[\"input_ids\"], encoder_hidden_states=encoder_outputs.last_hidden_state)\n",
    "print(decoder_outputs.keys())\n",
    "logits_ = model.lm_head(decoder_outputs.last_hidden_state * (model.model_dim**-0.5)) \n",
    "print(logits_[0])\n",
    "outputs = model(**inputs, labels=inputs.input_ids)\n",
    "print(outputs.keys())\n",
    "print(outputs.logits[0])\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 14.350248M\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "model = AutoModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "# print num params human readable\n",
    "print(f\"Number of parameters in model: {model.num_parameters()/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 124.439808M\n"
     ]
    }
   ],
   "source": [
    "# gpt2 \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "print(f\"Number of parameters in model: {model.num_parameters()/1000000}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n",
    "Let's check out distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 312])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"pooler_output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 14.350248M\n",
      "tensor([[[-0.3632,  0.1912, -0.1566,  ..., -0.0675, -0.0154,  0.0712],\n",
      "         [-0.1966, -0.0148,  0.2135,  ..., -0.5316,  0.3616, -0.2079],\n",
      "         [-0.0606, -0.0285, -0.0152,  ...,  0.0132, -0.8960,  0.3656],\n",
      "         ...,\n",
      "         [-0.3456,  0.2578, -0.2698,  ...,  0.0631,  0.2830,  0.3263],\n",
      "         [-0.1478,  0.3253, -0.1482,  ...,  0.0215,  0.9744,  0.1024],\n",
      "         [-0.0326, -0.2293, -0.0655,  ..., -0.2141, -0.5240,  0.3316]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.0233,  0.3683,  0.1870,  ...,  0.3800,  0.7760,  0.3710],\n",
      "         [-0.0709, -0.1214,  0.0082,  ..., -0.1908,  0.2418, -0.2694],\n",
      "         [-0.0662, -0.7025, -0.5034,  ..., -0.8311, -0.4651, -0.0320],\n",
      "         ...,\n",
      "         [-0.0476, -0.4128, -0.1295,  ..., -0.3573,  0.1723, -0.0999],\n",
      "         [ 0.0017,  0.2796,  0.6235,  ...,  0.4129,  0.6510, -0.1558],\n",
      "         [ 0.0803,  0.7502,  0.5556,  ...,  0.0634, -0.0539,  0.0026]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.0207,  0.3659, -0.1170,  ...,  0.5495,  0.1277,  1.8621],\n",
      "         [ 0.0668,  0.2000, -0.0230,  ...,  0.2839, -0.1221,  0.6510],\n",
      "         [ 0.0706, -0.2067, -0.4593,  ..., -0.1448, -0.6230,  0.5412],\n",
      "         ...,\n",
      "         [ 0.0381,  0.1300, -0.0644,  ...,  0.0937,  0.0160,  0.6169],\n",
      "         [ 0.0512,  0.4332,  0.3746,  ...,  0.2293,  0.2146,  0.5159],\n",
      "         [ 0.0867,  0.9581,  0.6868,  ...,  0.2452,  0.0117,  0.2042]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "model = AutoModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "# print num params human readable\n",
    "print(f\"Number of parameters in model: {model.num_parameters()/1000000}M\")\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state)    \n",
    "model2 = AutoModelForMaskedLM.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "outputs2 = model2(**inputs, labels=inputs[\"input_ids\"])\n",
    "print(outputs2.logits)\n",
    "model3 = AutoModelForCausalLM.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", is_decoder=True)\n",
    "outputs3 = model3(**inputs, labels=inputs[\"input_ids\"])\n",
    "print(outputs3.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "Let's check out gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5\n",
    "Let's check out T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
    "\n",
    "encoder_ouputs = encoder(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0235,  0.0490, -0.1531,  ..., -0.1444, -0.0982, -0.0988],\n",
      "         [ 0.2134,  0.0752, -0.2040,  ..., -0.1085, -0.1076, -0.4276],\n",
      "         [-0.1410,  0.0597,  0.2624,  ...,  0.1803,  0.2338, -0.1288],\n",
      "         ...,\n",
      "         [-0.1496, -0.0140, -0.2103,  ...,  0.0344, -0.0903,  0.1334],\n",
      "         [-0.2262,  0.0248, -0.0235,  ..., -0.1075,  0.0131,  0.0612],\n",
      "         [ 0.2149,  0.0967, -0.0164,  ..., -0.0685,  0.1851,  0.0746]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[[-0.0235,  0.0490, -0.1531,  ..., -0.1444, -0.0982, -0.0988],\n",
      "         [ 0.2134,  0.0752, -0.2040,  ..., -0.1085, -0.1076, -0.4276],\n",
      "         [-0.1410,  0.0597,  0.2624,  ...,  0.1803,  0.2338, -0.1288],\n",
      "         ...,\n",
      "         [-0.1496, -0.0140, -0.2103,  ...,  0.0344, -0.0903,  0.1334],\n",
      "         [-0.2262,  0.0248, -0.0235,  ..., -0.1075,  0.0131,  0.0612],\n",
      "         [ 0.2149,  0.0967, -0.0164,  ..., -0.0685,  0.1851,  0.0746]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.encoder_last_hidden_state)\n",
    "print(encoder_ouputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -818.2573,  -210.1928,  -336.9014,  ..., -1344.4564,\n",
      "          -1345.2698, -1345.4575],\n",
      "         [ -999.7817,  -271.1569,  -446.1989,  ..., -1461.3430,\n",
      "          -1461.7225, -1461.9854],\n",
      "         [-1119.0250,  -337.6578,  -511.1584,  ..., -1698.6843,\n",
      "          -1702.1517, -1700.9644],\n",
      "         ...,\n",
      "         [-1058.8584,  -303.8443,  -486.7072,  ..., -1663.9822,\n",
      "          -1671.6787, -1671.0627],\n",
      "         [ -985.0807,  -206.8462,  -393.3148,  ..., -1544.1208,\n",
      "          -1549.5402, -1548.0869],\n",
      "         [ -673.4078,   -71.0885,  -243.6629,  ..., -1063.8423,\n",
      "          -1067.3953, -1066.8198]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[-36.1622,  -9.2893, -14.8891,  ..., -59.4171, -59.4531, -59.4614],\n",
      "         [-44.1845, -11.9835, -19.7194,  ..., -64.5828, -64.5996, -64.6112],\n",
      "         [-49.4544, -14.9225, -22.5902,  ..., -75.0719, -75.2252, -75.1727],\n",
      "         ...,\n",
      "         [-46.7954, -13.4281, -21.5096,  ..., -73.5383, -73.8785, -73.8512],\n",
      "         [-43.5348,  -9.1414, -17.3822,  ..., -68.2411, -68.4807, -68.4164],\n",
      "         [-29.7607,  -3.1417, -10.7685,  ..., -47.0156, -47.1727, -47.1472]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = model.decoder(input_ids=input_ids, encoder_hidden_states=encoder_ouputs.last_hidden_state)\n",
    "print(model.lm_head(decoder_outputs.last_hidden_state))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 32128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -818.2573,  -210.1928,  -336.9014,  ..., -1344.4564,\n",
       "          -1345.2698, -1345.4575],\n",
       "         [ -999.7817,  -271.1569,  -446.1989,  ..., -1461.3430,\n",
       "          -1461.7225, -1461.9854],\n",
       "         [-1119.0250,  -337.6578,  -511.1584,  ..., -1698.6843,\n",
       "          -1702.1517, -1700.9644],\n",
       "         ...,\n",
       "         [-1058.8584,  -303.8443,  -486.7072,  ..., -1663.9822,\n",
       "          -1671.6787, -1671.0627],\n",
       "         [ -985.0807,  -206.8462,  -393.3148,  ..., -1544.1208,\n",
       "          -1549.5402, -1548.0869],\n",
       "         [ -673.4078,   -71.0885,  -243.6629,  ..., -1063.8423,\n",
       "          -1067.3953, -1066.8198]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs = model.decoder(encoder_hidden_states=encoder_ouputs.last_hidden_state, input_ids=input_ids)\n",
    "decoder_outputs.last_hidden_state\n",
    "model.lm_head(decoder_outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-36.0586, -11.3399, -15.3354,  ..., -60.5524, -60.5518, -60.5525],\n",
      "         [-45.0051, -13.2414, -20.2115,  ..., -65.1386, -65.1400, -65.1418],\n",
      "         [-50.9481, -16.4054, -23.7251,  ..., -77.4390, -77.5915, -77.5295],\n",
      "         ...,\n",
      "         [-47.4272, -12.9864, -19.9328,  ..., -73.8682, -74.1522, -74.0732],\n",
      "         [-36.7683,  -5.9688, -14.8783,  ..., -55.6205, -55.8629, -55.8130],\n",
      "         [-30.0457,  -3.7978, -11.6187,  ..., -47.4286, -47.6093, -47.5897]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "text_encoder_model = T5EncoderModel.from_pretrained('t5-small')\n",
    "text_decoder_model = T5ForConditionalGeneration.from_pretrained('t5-small', is_decoder=True, add_cross_attention=True)\n",
    "inputs = tokenizer(\"Hello, my dog is cute but\", return_tensors=\"pt\")\n",
    "outputs = text_encoder_model(**inputs)\n",
    "outputs.last_hidden_state.shape\n",
    "# print(outputs.last_hidden_state)\n",
    "outputs = text_decoder_model(encoder_outputs=outputs, decoder_input_ids=inputs[\"input_ids\"])\n",
    "outputs.keys()\n",
    "# print(outputs.encoder_last_hidden_state)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Hello, Hello, my dog is cute but</s>\n"
     ]
    }
   ],
   "source": [
    "gen = text_decoder_model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "print(tokenizer.decode(gen[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Fill: I am...</s>\n"
     ]
    }
   ],
   "source": [
    "full_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "inputs = tokenizer(\"Fill: I am ...\", return_tensors=\"pt\")\n",
    "gen = full_model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=20)\n",
    "print(tokenizer.decode(gen[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "gen = model.generate(input_ids=inputs[\"input_ids\"], max_length=20)\n",
    "print(tokenizer.decode(gen[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 703,   75, 5765, 1599,    1],\n",
       "        [  20,   89,    1,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"abc stride\", \"def\"], return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out tokenization of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small')\n",
    "print(tokenizer.eos_token_id, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenizer.encode(\"<pad>\", return_tensors=\"pt\"), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkbgen\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m GSM\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m GSM(tokenizer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mt5-small\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/kb-generator/kbgen/data/datasets.py:210\u001b[0m, in \u001b[0;36mGSM.__init__\u001b[0;34m(self, path, seed, test_size, device, tokenizer, numerical_tokenizer)\u001b[0m\n\u001b[1;32m    201\u001b[0m types_to_nodes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m<dummy>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfields \u001b[39m=\u001b[39m Fields(\n\u001b[1;32m    204\u001b[0m     \u001b[39m# numerical=types_to_nodes[0],\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     numerical\u001b[39m=\u001b[39m[],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     text\u001b[39m=\u001b[39m[],\n\u001b[1;32m    209\u001b[0m )\n\u001b[0;32m--> 210\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    211\u001b[0m     df,\n\u001b[1;32m    212\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfields,\n\u001b[1;32m    213\u001b[0m     seed,\n\u001b[1;32m    214\u001b[0m     test_size,\n\u001b[1;32m    215\u001b[0m     device,\n\u001b[1;32m    216\u001b[0m     tokenizer,\n\u001b[1;32m    217\u001b[0m     numerical_tokenizer,\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    219\u001b[0m \u001b[39m### test\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconsistency_check()\n",
      "File \u001b[0;32m~/kb-generator/kbgen/data/datasets.py:35\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, df, fields, seed, test_size, device, tokenizer, numerical_tokenizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategorical_pad_token \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     34\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(seed)\n\u001b[0;32m---> 35\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(tokenizer, numerical_tokenizer, device)\n\u001b[1;32m     36\u001b[0m indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m), device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_idx \u001b[39m=\u001b[39m train_test_split(\n\u001b[1;32m     38\u001b[0m     indices, test_size\u001b[39m=\u001b[39mtest_size, random_state\u001b[39m=\u001b[39mseed\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/kb-generator/kbgen/data/datasets.py:69\u001b[0m, in \u001b[0;36mDataset.tokenize\u001b[0;34m(self, tokenizer, numerical_tokenizer, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategorical_pad_token \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpad_token_id, (\n\u001b[1;32m     60\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCategorical pad token \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategorical_pad_token\u001b[39m}\u001b[39;00m\u001b[39m does not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatch tokenizer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\u001b[39m'\u001b[39m\u001b[39m<pad>\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumerical_pad_token \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumerical_tokenizer(\u001b[39m\"\u001b[39m\u001b[39m<pad>\u001b[39m\u001b[39m\"\u001b[39m), (\n\u001b[1;32m     64\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumerical pad token \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumerical_pad_token\u001b[39m}\u001b[39;00m\u001b[39m does not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatch tokenizer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumerical_tokenizer(\u001b[39m'\u001b[39m\u001b[39m<pad>\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dict \u001b[39m=\u001b[39m TensorDict(\n\u001b[0;32m---> 69\u001b[0m     df_tokenize(\n\u001b[1;32m     70\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf,\n\u001b[1;32m     71\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfields,\n\u001b[1;32m     72\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m     73\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumerical_tokenizer,\n\u001b[1;32m     74\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     75\u001b[0m     ),\n\u001b[1;32m     76\u001b[0m     fields\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfields,\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dict\n",
      "File \u001b[0;32m~/kb-generator/kbgen/utils/utils.py:39\u001b[0m, in \u001b[0;36mdf_tokenize\u001b[0;34m(df, fields, categorical_tokenizer, numerical_tokenizer, device, preprocess_text)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m field \u001b[39min\u001b[39;00m fields[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m field \u001b[39min\u001b[39;00m fields[\u001b[39m\"\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     38\u001b[0m     text \u001b[39m=\u001b[39m df[field]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m---> 39\u001b[0m     text \u001b[39m=\u001b[39m categorical_tokenizer(\n\u001b[1;32m     40\u001b[0m         text, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m     tensor_dict[field] \u001b[39m=\u001b[39m text[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m \u001b[39m# elif field in fields[\"categorical\"]:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m#     values = df[field].values\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m#     values = torch.cat(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m#     )\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m#     tensor_dict[field] = values.to(device)\u001b[39;00m\n",
      "File \u001b[0;32m/transformers/src/transformers/tokenization_utils_base.py:2581\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2580\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2581\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2582\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2583\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/transformers/src/transformers/tokenization_utils_base.py:2639\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2636\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2638\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2639\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2640\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2641\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2642\u001b[0m     )\n\u001b[1;32m   2644\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2645\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2647\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2648\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "from kbgen.data.datasets import GSM\n",
    "dataset = GSM(tokenizer=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i-mobile': 11,\n",
       " 'LG': 18,\n",
       " 'Oppo': 30,\n",
       " 'VK Mobile': 57,\n",
       " 'Archos': 81,\n",
       " 'Karbonn': 95}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for k, v in dataset.categories_str_to_id[\"phone.oem\"].items() if v in [11, 95, 18, 81, 30 ,57]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_from_token_decoder, item_from_prototypes in zip(proto_str_to_id[field], examples):\n",
    "    assert item_from_token_decoder == item_from_prototypes, f\"{item_from_token_decoder.replace(' ', '.')} != {item_from_prototypes.replace(' ', '.')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('', 0), ('No', 1), ('Yes', 2), ('Class 6', 3)], 51)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.categories_str_to_id[\"phone.network_edge\"].items())[:4], len(dataset.categories_str_to_id[\"phone.network_edge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'No': 1,\n",
       " 'Yes': 2,\n",
       " 'Class 6': 3,\n",
       " 'Class 10': 4,\n",
       " 'Up to 236.8 kbps': 5,\n",
       " 'Class 33': 6,\n",
       " 'Class 12': 7,\n",
       " 'Yes (SIM 1 only)': 8,\n",
       " 'Class 32': 9,\n",
       " 'Up to 560 kbps': 10,\n",
       " 'Up to 384 kbps': 11,\n",
       " 'Yes - 3G model': 12,\n",
       " 'Class 12 (T-Mobile)': 13,\n",
       " 'Up to 237 kbps': 14,\n",
       " 'Class 11': 15,\n",
       " 'Up to 236 kbps': 16,\n",
       " 'Class 12, 296 / 177.6 kbits': 17,\n",
       " 'Class 4': 18,\n",
       " 'Up to 60 kbps': 19,\n",
       " 'Up to 296 kbps': 20,\n",
       " 'Class B': 21,\n",
       " 'Class 6 (Up to 177.6 kbps)': 22,\n",
       " 'Class 32, 296 / 177.6 kbits': 23,\n",
       " 'Class 32, 296 kbits': 24,\n",
       " 'Class 32, 296 / 178.8 kbits': 25,\n",
       " 'Class 32, 296 kbps': 26,\n",
       " 'Class 6 (downlink only)': 27,\n",
       " 'Class 32, 296 kbps; DTM Class 11, 177 kbps': 28,\n",
       " 'Class 32, 296 kbps; DTM Class 11, 178.8 kbps': 29,\n",
       " 'Yes, DL only': 30,\n",
       " 'Class 8': 31,\n",
       " 'Class 32, 236.8 kbits': 32,\n",
       " 'Class 32, up to 177 kbits': 33,\n",
       " 'Class 11, 236.8 kbps': 34,\n",
       " 'Class 32, 296 kbps; DTM Class 11, 236.8 kbps': 35,\n",
       " 'Yes, 118.4 kbps': 36,\n",
       " 'Up to 480 kbps': 37,\n",
       " 'Yes - SCH-I605, SPH-L900': 38,\n",
       " 'Yes -SGH-T779, SGH-I497': 39,\n",
       " 'Class 12 (SIM 1)': 40,\n",
       " 'Class 10 (SIM 1, download only)': 41,\n",
       " 'Class 10 (D600E only)': 42,\n",
       " 'Class 10 (E340E only)': 43,\n",
       " 'Yes (in D500E version only)': 44,\n",
       " 'Up to 177 kbps': 45,\n",
       " 'Up to 200 kbps': 46,\n",
       " 'Up to 225 kbps': 47,\n",
       " 'Up to 247 kbps': 48,\n",
       " 'W958c only': 49,\n",
       " 'Class 12 (4+1/3+2 slots), 180 - 230 kbps': 50}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.categories_str_to_id[\"phone.network_edge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
